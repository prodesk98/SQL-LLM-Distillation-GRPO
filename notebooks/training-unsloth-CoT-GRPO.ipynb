{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T01:59:16.839448Z",
     "start_time": "2025-05-20T01:59:16.834651Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "from trl import SFTTrainer, SFTConfig, GRPOConfig, GRPOTrainer\n",
    "import duckdb\n",
    "import re"
   ],
   "id": "5665a5b451363cb4",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load the dataset",
   "id": "7887bf9886c781e6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T01:59:20.011538Z",
     "start_time": "2025-05-20T01:59:16.842178Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DATASET_REPO_ID = \"proton98/sql-distill-llama-3-1-70b-instruct-reasoning\"\n",
    "train_dataset = load_dataset(DATASET_REPO_ID, split=\"train[:100]\")"
   ],
   "id": "357ae22d05c03713",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Prompt template",
   "id": "4a3fb6be01916a01"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T01:59:20.018228Z",
     "start_time": "2025-05-20T01:59:20.013174Z"
    }
   },
   "cell_type": "code",
   "source": [
    "REASONING_START = \"<think>\"\n",
    "REASONING_END = \"</think>\"\n",
    "SOLUTION_START = \"<sql>\"\n",
    "SOLUTION_END = \"</sql>\"\n",
    "\n",
    "SYSTEM_PROMPT_TEMPLATE = \\\n",
    "f\"\"\"You are an expert in writing optimized SQL queries.\n",
    "Think about the problem and provide your working out.\n",
    "Place it between {REASONING_START} and {REASONING_END}.\n",
    "Then, provide your solution between {SOLUTION_START} and {SOLUTION_END}.\n",
    "\n",
    "Context:\n",
    "---\n",
    "{{context}}\n",
    "---\"\"\".rstrip()"
   ],
   "id": "d32eedcd72de965d",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Process the dataset",
   "id": "c3186982cc4bb69a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T01:59:20.075956Z",
     "start_time": "2025-05-20T01:59:20.021791Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sql_match_format = re.compile(rf\"{SOLUTION_START}(.*?){SOLUTION_END}\", re.DOTALL)\n",
    "reasoning_match_format = re.compile(rf\"{REASONING_START}(.*?){REASONING_END}\", re.DOTALL)\n",
    "\n",
    "def extract_sql(text: str) -> str | None:\n",
    "    \"\"\"\n",
    "    Extracts the SQL using regex from the generated text.\n",
    "    :param text:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    sql_match = sql_match_format.search(text)\n",
    "    if sql_match:\n",
    "        return sql_match.group(1).strip()\n",
    "\n",
    "\n",
    "def extract_think(text: str) -> str | None:\n",
    "    \"\"\"\n",
    "    Extracts the think using regex from the generated text.\n",
    "    :param text:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    think_match = reasoning_match_format.search(text)\n",
    "    if think_match:\n",
    "        return think_match.group(1).strip()\n",
    "\n",
    "\n",
    "def correct_reasoning_format(content: str) -> str:\n",
    "    \"\"\"\n",
    "    Formatting prompt for exact match.\n",
    "    :param content:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    __think = extract_think(content)\n",
    "    __sql = extract_sql(content)\n",
    "    return f\"\"\"{REASONING_START}\\n{__think}\\n{REASONING_END}\\n{SOLUTION_START}\\n{__sql}\\n{SOLUTION_END}\"\"\"\n",
    "\n",
    "\n",
    "def conversations_formatting(dataset: DatasetDict) -> Dataset | DatasetDict:\n",
    "    \"\"\"\n",
    "    Format the conversations for the model.\n",
    "    :param dataset:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    dataset = dataset.map(lambda x: {\n",
    "        \"prompt\": [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT_TEMPLATE.format(context=x[\"sql_context\"]).strip()},\n",
    "            {\"role\": \"user\", \"content\": x[\"sql_prompt\"]},\n",
    "        ],\n",
    "        \"questions\": x[\"sql_prompt\"],\n",
    "        \"contexts\": x[\"sql_context\"],\n",
    "        \"answers\": extract_sql(x[\"generation\"]),\n",
    "    }, remove_columns=[\n",
    "        k for k in dataset.column_names if k not in [\"questions\", \"contexts\", \"answers\", \"prompt\"]\n",
    "    ])\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def conversations_supervised_fine_tuning(dataset: DatasetDict) -> Dataset | DatasetDict:\n",
    "    \"\"\"\n",
    "    Format the conversations for supervised fine-tuning.\n",
    "    :param dataset:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    dataset = dataset.map(lambda x: {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT_TEMPLATE.format(context=x[\"sql_context\"]).strip()},\n",
    "            {\"role\": \"user\", \"content\": x[\"sql_prompt\"]},\n",
    "            {\"role\": \"assistant\", \"content\": correct_reasoning_format(x[\"generation\"])},\n",
    "        ],\n",
    "    }, remove_columns=[\n",
    "        k for k in dataset.column_names if k not in [\"messages\"]\n",
    "    ])\n",
    "    return dataset\n",
    "\n",
    "\n",
    "train_dataset_grpo = conversations_formatting(train_dataset)\n",
    "train_dataset_sft = conversations_supervised_fine_tuning(train_dataset)"
   ],
   "id": "7ed4137dc0cb0a5a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 100/100 [00:00<00:00, 5529.66 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 6772.98 examples/s]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load the model and tokenizer",
   "id": "aa0a5180d6306fe6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T02:01:04.805908Z",
     "start_time": "2025-05-20T01:59:20.077887Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MODEL_NAME = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "MAX_SEQ_LEN = 4096\n",
    "MAX_LORA_RANK = 16\n",
    "LORA_RANK = 16\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = MODEL_NAME,\n",
    "    max_seq_length = MAX_SEQ_LEN,\n",
    "    load_in_4bit = True,\n",
    "    fast_inference = True,\n",
    "    max_lora_rank = MAX_LORA_RANK,\n",
    "    gpu_memory_utilization = 0.7,\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = LORA_RANK,\n",
    "    lora_alpha = LORA_RANK,\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,\n",
    ")"
   ],
   "id": "320f38bf6b87e9a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.5.6: Fast Llama patching. Transformers: 4.51.3. vLLM: 0.8.5.post1.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3060. Num GPUs = 1. Max memory: 12.0 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: vLLM loading unsloth/llama-3.2-3b-instruct-unsloth-bnb-4bit with actual GPU utilization = 63.86%\n",
      "Unsloth: Your GPU has CUDA compute capability 8.6 with VRAM = 12.0 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 4096. Num Sequences = 192.\n",
      "Unsloth: vLLM's KV Cache can use up to 5.24 GB. Also swap space = 0 GB.\n",
      "INFO 05-20 01:59:33 [config.py:717] This model supports multiple tasks: {'score', 'embed', 'generate', 'reward', 'classify'}. Defaulting to 'generate'.\n",
      "INFO 05-20 01:59:33 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=4096.\n",
      "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection', 'model.layers.1.mlp'], 'llm_int8_threshold': 6.0}\n",
      "INFO 05-20 01:59:34 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='unsloth/llama-3.2-3b-instruct-unsloth-bnb-4bit', speculative_config=None, tokenizer='unsloth/llama-3.2-3b-instruct-unsloth-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=bitsandbytes, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda:0, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=unsloth/llama-3.2-3b-instruct-unsloth-bnb-4bit, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"backend\":\"inductor\",\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"debug\":false,\"dce\":true,\"coordinate_descent_tuning\":true,\"trace.enabled\":false,\"trace.graph_diagram\":false,\"triton.cudagraphs\":true,\"compile_threads\":48,\"max_autotune\":false,\"disable_progress\":false,\"verbose_progress\":true,\"enable_auto_functionalized_v2\":false},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n",
      "WARNING 05-20 01:59:35 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f11c985f070>\n",
      "INFO 05-20 01:59:35 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "WARNING 05-20 01:59:35 [interface.py:314] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "INFO 05-20 01:59:35 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "WARNING 05-20 01:59:35 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 05-20 01:59:35 [gpu_model_runner.py:1329] Starting to load model unsloth/llama-3.2-3b-instruct-unsloth-bnb-4bit...\n",
      "INFO 05-20 01:59:36 [loader.py:1187] Loading weights with BitsAndBytes quantization. May take a while ...\n",
      "INFO 05-20 01:59:40 [weight_utils.py:265] Using model weights format ['*.safetensors']\n",
      "INFO 05-20 01:59:41 [weight_utils.py:315] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:02<00:00,  2.87s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:02<00:00,  2.87s/it]\n",
      "\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.27it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-20 01:59:45 [punica_selector.py:18] Using PunicaWrapperGPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-20 01:59:45 [gpu_model_runner.py:1347] Model loading took 2.2969 GiB and 9.422128 seconds\n",
      "INFO 05-20 01:59:59 [backends.py:420] Using cache directory: /root/.cache/vllm/torch_compile_cache/a11df5d034/rank_0_0 for vLLM's torch.compile\n",
      "INFO 05-20 01:59:59 [backends.py:430] Dynamo bytecode transform time: 13.82 s\n",
      "INFO 05-20 02:00:07 [backends.py:118] Directly load the compiled graph(s) for shape None from the cache, took 6.871 s\n",
      "INFO 05-20 02:00:11 [monitor.py:33] torch.compile takes 13.82 s in total\n",
      "INFO 05-20 02:00:11 [kv_cache_utils.py:634] GPU KV cache size: 31,472 tokens\n",
      "INFO 05-20 02:00:11 [kv_cache_utils.py:637] Maximum concurrency for 4,096 tokens per request: 7.68x\n",
      "INFO 05-20 02:00:58 [gpu_model_runner.py:1686] Graph capturing finished in 46 secs, took 1.19 GiB\n",
      "INFO 05-20 02:00:58 [core.py:159] init engine (profile, create kv cache, warmup model) took 72.86 seconds\n",
      "Unsloth: Just some info: will skip parsing ['q_norm', 'pre_feedforward_layernorm', 'post_feedforward_layernorm', 'k_norm']\n",
      "Unsloth: Just some info: will skip parsing ['q_norm', 'pre_feedforward_layernorm', 'post_feedforward_layernorm', 'k_norm']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.5.6 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Apply Chat Template",
   "id": "30f53e89e131465d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T02:01:05.014821Z",
     "start_time": "2025-05-20T02:01:04.807585Z"
    }
   },
   "cell_type": "code",
   "source": "train_dataset_sft = train_dataset_sft.map(lambda x: {\"text\": tokenizer.apply_chat_template(x[\"messages\"], batched=True, tokenize = False)})",
   "id": "415dee64bc496ea2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 100/100 [00:00<00:00, 3510.20 examples/s]\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T02:01:05.025660Z",
     "start_time": "2025-05-20T02:01:05.016352Z"
    }
   },
   "cell_type": "code",
   "source": "train_dataset_sft[0]",
   "id": "7b1b7cf49f987e7b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'content': \"You are an expert in writing optimized SQL queries.\\nThink about the problem and provide your working out.\\nPlace it between <think> and </think>.\\nThen, provide your solution between <sql> and </sql>.\\n\\nContext:\\n---\\nCREATE TABLE salesperson (salesperson_id INT, name TEXT, region TEXT); INSERT INTO salesperson (salesperson_id, name, region) VALUES (1, 'John Doe', 'North'), (2, 'Jane Smith', 'South'); CREATE TABLE timber_sales (sales_id INT, salesperson_id INT, volume REAL, sale_date DATE); INSERT INTO timber_sales (sales_id, salesperson_id, volume, sale_date) VALUES (1, 1, 120, '2021-01-01'), (2, 1, 150, '2021-02-01'), (3, 2, 180, '2021-01-01');\\n---\",\n",
       "   'role': 'system'},\n",
       "  {'content': 'What is the total volume of timber sold by each salesperson, sorted by salesperson?',\n",
       "   'role': 'user'},\n",
       "  {'content': '<think>\\nTo answer the question of what the total volume of timber sold by each salesperson is, sorted by salesperson, we need to follow a series of logical steps.\\n\\nFirst, we must identify the tables that contain the relevant information. In this case, we have two tables: `salesperson` and `timber_sales`. The `salesperson` table contains information about each salesperson, including their unique identifier (`salesperson_id`) and their name. The `timber_sales` table contains information about each sale, including the volume of timber sold and the identifier of the salesperson who made the sale.\\n\\nNext, we must determine how to combine these two tables to get the desired information. Since each sale is associated with a specific salesperson, we can join the `timber_sales` table with the `salesperson` table on the `salesperson_id` column. This will allow us to link each sale with the corresponding salesperson.\\n\\nAfter joining the tables, we need to calculate the total volume of timber sold by each salesperson. To do this, we can use the `SUM` aggregation function on the `volume` column, grouping the results by `salesperson_id` and `name`. This will give us the total volume sold by each salesperson.\\n\\nFinally, we need to sort the results by salesperson. Since the problem statement does not specify a particular order, we will assume that the results should be sorted in ascending order by `name`.\\n\\nHowever, the objective SQL query is sorting the results by `total_volume` in descending order. Therefore, we will adjust our final query accordingly.\\n</think>\\n<sql>\\nSELECT salesperson_id, name, SUM(volume) as total_volume FROM timber_sales JOIN salesperson ON timber_sales.salesperson_id = salesperson.salesperson_id GROUP BY salesperson_id, name ORDER BY name ASC;\\n</sql>',\n",
       "   'role': 'assistant'}],\n",
       " 'text': \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 20 May 2025\\n\\nYou are an expert in writing optimized SQL queries.\\nThink about the problem and provide your working out.\\nPlace it between <think> and </think>.\\nThen, provide your solution between <sql> and </sql>.\\n\\nContext:\\n---\\nCREATE TABLE salesperson (salesperson_id INT, name TEXT, region TEXT); INSERT INTO salesperson (salesperson_id, name, region) VALUES (1, 'John Doe', 'North'), (2, 'Jane Smith', 'South'); CREATE TABLE timber_sales (sales_id INT, salesperson_id INT, volume REAL, sale_date DATE); INSERT INTO timber_sales (sales_id, salesperson_id, volume, sale_date) VALUES (1, 1, 120, '2021-01-01'), (2, 1, 150, '2021-02-01'), (3, 2, 180, '2021-01-01');\\n---<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhat is the total volume of timber sold by each salesperson, sorted by salesperson?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n<think>\\nTo answer the question of what the total volume of timber sold by each salesperson is, sorted by salesperson, we need to follow a series of logical steps.\\n\\nFirst, we must identify the tables that contain the relevant information. In this case, we have two tables: `salesperson` and `timber_sales`. The `salesperson` table contains information about each salesperson, including their unique identifier (`salesperson_id`) and their name. The `timber_sales` table contains information about each sale, including the volume of timber sold and the identifier of the salesperson who made the sale.\\n\\nNext, we must determine how to combine these two tables to get the desired information. Since each sale is associated with a specific salesperson, we can join the `timber_sales` table with the `salesperson` table on the `salesperson_id` column. This will allow us to link each sale with the corresponding salesperson.\\n\\nAfter joining the tables, we need to calculate the total volume of timber sold by each salesperson. To do this, we can use the `SUM` aggregation function on the `volume` column, grouping the results by `salesperson_id` and `name`. This will give us the total volume sold by each salesperson.\\n\\nFinally, we need to sort the results by salesperson. Since the problem statement does not specify a particular order, we will assume that the results should be sorted in ascending order by `name`.\\n\\nHowever, the objective SQL query is sorting the results by `total_volume` in descending order. Therefore, we will adjust our final query accordingly.\\n</think>\\n<sql>\\nSELECT salesperson_id, name, SUM(volume) as total_volume FROM timber_sales JOIN salesperson ON timber_sales.salesperson_id = salesperson.salesperson_id GROUP BY salesperson_id, name ORDER BY name ASC;\\n</sql><|eot_id|>\"}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Supervised fine-tuning",
   "id": "188166f599adb442"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T02:02:10.294153Z",
     "start_time": "2025-05-20T02:01:05.028145Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sft_trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset_sft,\n",
    "    args = SFTConfig(\n",
    "        dataset_text_field = \"text\",\n",
    "        per_device_train_batch_size = 8,\n",
    "        gradient_accumulation_steps = 1,\n",
    "        warmup_steps = 5,\n",
    "        num_train_epochs = 1,\n",
    "        learning_rate = 2e-4,\n",
    "        logging_steps = 5,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        report_to = \"none\",\n",
    "    ),\n",
    ")\n",
    "sft_trainer.train()"
   ],
   "id": "20385174fb1dd5f1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Tokenizing [\"text\"] (num_proc=12): 100%|██████████| 100/100 [00:03<00:00, 28.10 examples/s]\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 100 | Num Epochs = 1 | Total steps = 13\n",
      "O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 1\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 1 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 24,313,856/3,000,000,000 (0.81% trained)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:53, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.514400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.210800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=13, training_loss=1.2801625178410456, metrics={'train_runtime': 58.8642, 'train_samples_per_second': 1.699, 'train_steps_per_second': 0.221, 'total_flos': 556659810385920.0, 'train_loss': 1.2801625178410456})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T02:02:21.230819Z",
     "start_time": "2025-05-20T02:02:10.296640Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = tokenizer.apply_chat_template(\n",
    "    train_dataset_sft[0][\"messages\"][:2],\n",
    "    tokenize = False,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    ")\n",
    "from transformers import TextStreamer\n",
    "_ = model.generate(\n",
    "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
    "    temperature = 0,\n",
    "    max_new_tokens = 1024,\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt = False),\n",
    ")"
   ],
   "id": "32ce5c134399902",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 20 May 2025\n",
      "\n",
      "You are an expert in writing optimized SQL queries.\n",
      "Think about the problem and provide your working out.\n",
      "Place it between <think> and </think>.\n",
      "Then, provide your solution between <sql> and </sql>.\n",
      "\n",
      "Context:\n",
      "---\n",
      "CREATE TABLE salesperson (salesperson_id INT, name TEXT, region TEXT); INSERT INTO salesperson (salesperson_id, name, region) VALUES (1, 'John Doe', 'North'), (2, 'Jane Smith', 'South'); CREATE TABLE timber_sales (sales_id INT, salesperson_id INT, volume REAL, sale_date DATE); INSERT INTO timber_sales (sales_id, salesperson_id, volume, sale_date) VALUES (1, 1, 120, '2021-01-01'), (2, 1, 150, '2021-02-01'), (3, 2, 180, '2021-01-01');\n",
      "---<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is the total volume of timber sold by each salesperson, sorted by salesperson?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "<think>\n",
      "To solve this problem, we need to join the salesperson table with the timber_sales table based on the salesperson_id. We then need to calculate the total volume of timber sold by each salesperson. To do this, we can use the SUM function to sum up the volume of timber sold by each salesperson. Finally, we need to sort the results by salesperson.\n",
      "</think>\n",
      "<sql>\n",
      "SELECT salesperson_id, name, SUM(volume) AS total_volume FROM timber_sales GROUP BY salesperson_id ORDER BY salesperson_id;\n",
      "</sql><|eot_id|>\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T02:02:21.599595Z",
     "start_time": "2025-05-20T02:02:21.232897Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "del train_dataset_sft\n",
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()"
   ],
   "id": "1d73c9996406d915",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "373"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Reward functions",
   "id": "36b95079a0825fca"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T02:02:21.615533Z",
     "start_time": "2025-05-20T02:02:21.601584Z"
    }
   },
   "cell_type": "code",
   "source": [
    "COLORED_GREEN = \"\\033[92m\"\n",
    "COLORED_BLUE = \"\\033[94m\"\n",
    "COLORED_RESET = \"\\033[0m\"\n",
    "BOLD = \"\\033[1m\"\n",
    "\n",
    "response_match_format = re.compile(\n",
    "    rf\"^{REASONING_START}\\n.*?\\n{REASONING_END}\\n{SOLUTION_START}\\n.*?\\n{SOLUTION_END}$\",\n",
    "    re.DOTALL | re.MULTILINE\n",
    ")\n",
    "\n",
    "\n",
    "def match_format_exactly(completions: list[list[dict[str, str]]], **kwargs) -> list[float]: # noqa\n",
    "    \"\"\"\n",
    "    Check if the completions match the expected format exactly.\n",
    "    :param completions:\n",
    "    :param kwargs:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    scores: list[float] = []\n",
    "    for completion in completions:\n",
    "        score = 0\n",
    "        response = completion[0][\"content\"]\n",
    "        if response_match_format.search(response) is not None: score += 1.1\n",
    "        scores.append(score)\n",
    "    return scores\n",
    "\n",
    "\n",
    "def match_format_approximately(completions: list[list[dict[str, str]]], **kwargs) -> list[float]: # noqa\n",
    "    \"\"\"\n",
    "    Check if the completions match the expected format approximately.\n",
    "    :param completions:\n",
    "    :param kwargs:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    scores: list[float] = []\n",
    "    for completion in completions:\n",
    "        score = 0\n",
    "        response = completion[0][\"content\"]\n",
    "        score += 0.125 if response.count(f\"{REASONING_START}\\n\")    == 1 else -1.0\n",
    "        score += 0.125 if response.count(f\"\\n{REASONING_END}\\n\")    == 1 else -1.0\n",
    "        score += 0.125 if response.count(f\"\\n{SOLUTION_START}\\n\")   == 1 else -1.0\n",
    "        score += 0.125 if response.count(f\"\\n{SOLUTION_END}\")       == 1 else -1.0\n",
    "        scores.append(score)\n",
    "    return scores\n",
    "\n",
    "\n",
    "def validate_sql_query(sql_query: str, context: str) -> tuple[bool, str]:\n",
    "    \"\"\"\n",
    "    Validate the SQL query against the provided context.\n",
    "\n",
    "    Args:\n",
    "        sql_query (str): The SQL query to validate.\n",
    "        context (str): The context in which the SQL query is executed\n",
    "            (e.g., table schema, data).\n",
    "    Returns:\n",
    "        tuple[bool, str]: Whether the SQL query is valid.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        con = duckdb.connect(database=':memory:')\n",
    "\n",
    "        context_statements = [stmt.strip() for stmt in context.strip().split(';') if stmt.strip()]\n",
    "        for statement in context_statements:\n",
    "            con.execute(statement)\n",
    "\n",
    "        con.execute(sql_query)\n",
    "        _ = con.fetchall()\n",
    "\n",
    "        return True, \"SQL query is valid and executed successfully.\"\n",
    "    except Exception as e:\n",
    "        return False, str(e)\n",
    "\n",
    "\n",
    "def check_sql_reward(\n",
    "    completions: list[list[dict[str, str]]],\n",
    "    questions: list[str],\n",
    "    answers: list[str],\n",
    "    contexts: list[str],\n",
    "    **kwargs # noqa\n",
    ") -> list[float]:\n",
    "    \"\"\"\n",
    "    Check the SQL reward for the given prompts and completions.\n",
    "    :param completions:\n",
    "    :param questions:\n",
    "    :param answers:\n",
    "    :param contexts:\n",
    "    :param kwargs:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    pred_responses: list[str | None] = [extract_sql(r) for r in responses]\n",
    "\n",
    "    print(\n",
    "        '-'*20,\n",
    "        f\"{BOLD}Question:{COLORED_RESET} {questions[0]}{COLORED_RESET}\",\n",
    "        f\"\\n{BOLD}Response:{COLORED_RESET} \\n{COLORED_GREEN}{responses[0]}{COLORED_RESET}\",\n",
    "        f\"\\n{BOLD}Real Answer:{COLORED_RESET} \\n{COLORED_GREEN}{answers[0]}{COLORED_RESET}\",\n",
    "        f\"\\n{BOLD}Extracted:{COLORED_RESET} \\n\"\n",
    "        f\"{COLORED_BLUE}{pred_responses[0] if pred_responses[0] is not None else '-Invalid Format-'}{COLORED_RESET}\",\n",
    "    )\n",
    "\n",
    "    scores: list[float] = []\n",
    "    for pred, context, answer in zip(pred_responses, contexts, answers):\n",
    "        score = 0\n",
    "        if pred is None:\n",
    "            scores.append(0)\n",
    "            continue\n",
    "\n",
    "        # Check if the SQL query is valid\n",
    "        is_valid, _ = validate_sql_query(pred, context)\n",
    "\n",
    "        # Check if the SQL query is similar to the true SQL\n",
    "        # Correct answer gets 3 points\n",
    "        if is_valid:\n",
    "            # Exact match\n",
    "            if pred == answer: score += 3.0\n",
    "            # Match if spaces are seen\n",
    "            elif pred.strip() == answer.strip(): score += 1.5\n",
    "        else: score -= 1.5 # Penalty for incorrect SQL\n",
    "        scores.append(score)\n",
    "    return scores"
   ],
   "id": "ee83b92c68292daa",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training",
   "id": "8a61508e24236688"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T02:02:21.650187Z",
     "start_time": "2025-05-20T02:02:21.619277Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MAX_PROMPT_LEN = 2048\n",
    "MAX_COMPLETION_LEN = 1024\n",
    "\n",
    "training_args = GRPOConfig(\n",
    "    use_vllm = True,\n",
    "    learning_rate = 5e-6,\n",
    "    adam_beta1 = 0.9,\n",
    "    adam_beta2 = 0.99,\n",
    "    weight_decay = 0.1,\n",
    "    warmup_ratio = 0.1,\n",
    "    lr_scheduler_type = \"cosine\",\n",
    "    optim = \"paged_adamw_8bit\",\n",
    "    logging_steps = 1,\n",
    "    bf16 = is_bfloat16_supported(),\n",
    "    fp16 = not is_bfloat16_supported(),\n",
    "    per_device_train_batch_size = 8,\n",
    "    gradient_accumulation_steps = 1,\n",
    "    num_generations = 4,\n",
    "    max_prompt_length = MAX_PROMPT_LEN,\n",
    "    max_completion_length = MAX_COMPLETION_LEN,\n",
    "    max_steps = 500,\n",
    "    save_steps = 150,\n",
    "    max_grad_norm = 0.1,\n",
    "    report_to = \"none\",\n",
    "    output_dir = \"outputs\",\n",
    ")"
   ],
   "id": "7e7c4c7a0b399b57",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "grpo_trainer = GRPOTrainer(\n",
    "    model = model,\n",
    "    processing_class = tokenizer,\n",
    "    reward_funcs = [ # type: ignore\n",
    "        match_format_exactly,\n",
    "        match_format_approximately,\n",
    "        check_sql_reward,\n",
    "    ],\n",
    "    args = training_args,\n",
    "    train_dataset = train_dataset_grpo,\n",
    ")\n",
    "grpo_trainer.train()"
   ],
   "id": "1e5a2b0bb800517c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T02:05:44.844713Z",
     "start_time": "2025-05-20T02:05:44.844493Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "123bc0cf3367eded",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
