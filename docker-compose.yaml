

services:
  train:
    build:
      context: .
      dockerfile: dockers/train.Dockerfile
    env_file:
    - .env
    - .train
    environment:
    - NVIDIA_VISIBLE_DEVICES=all
    - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ gpu ]
    volumes:
      - .cache/huggingface/:/root/.cache/huggingface/
    ports:
    - "8265:8265"

  distill:
    build:
      context: .
      dockerfile: dockers/distill.Dockerfile
    env_file:
      - .env
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - VLLM_WORKER_MULTIPROC_METHOD=spawn
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ gpu ]
    ports:
      - "8265:8265"
    command: >
      uv run main.py distill
      --limit 100
      --publish
      --dataset-repo-id gretelai/synthetic_text_to_sql
      --publish-repo-id ${NAMESPACE}/sql-distill-Llama-3.2-1B-reasoning
      --provider HuggingFace
      --model meta-llama/Llama-3.2-1B
      --validate
      --private-repo
      --batch-size 64

  test:
    build:
      context: .
      dockerfile: dockers/distill.Dockerfile
    env_file:
      - .env
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ gpu ]
    command: >
      tail -f /dev/null